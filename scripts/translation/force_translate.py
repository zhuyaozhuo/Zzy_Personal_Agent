#!/usr/bin/env python3
"""
å¼ºåˆ¶é‡æ–°ç¿»è¯‘è‹±æ–‡å­—å¹•ä¸ºä¸­æ–‡
"""

import json
import os
import re
import sys
from pathlib import Path
from datetime import datetime
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from docx.oxml import OxmlElement

sys.path.insert(0, str(Path(__file__).parent))

from core.config import settings
from utils.logger import logger


def set_chinese_font(run, font_name='SimSun', font_size=12):
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    run.font.name = font_name
    run.font.size = Pt(font_size)
    r = run._element
    rPr = r.get_or_add_rPr()
    rFonts = OxmlElement('w:rFonts')
    rFonts.set(qn('w:eastAsia'), font_name)
    rPr.insert(0, rFonts)


def auto_sentence_break(text: str) -> str:
    """è‡ªåŠ¨æ–­å¥å¤„ç†"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'([.!?])\s+', r'\1\n\n', text)
    text = re.sub(r'([ã€‚ï¼ï¼Ÿ])', r'\1\n\n', text)
    text = re.sub(r'([,ï¼Œ])\s*', r'\1 ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def translate_text(text: str, chunk_size: int = 3000) -> str:
    """ä½¿ç”¨GLM APIç¿»è¯‘æ–‡æœ¬"""
    from langchain_zhipu import ChatZhipuAI
    from langchain_core.messages import HumanMessage
    
    llm = ChatZhipuAI(
        model=settings.ZHIPU_MODEL,
        temperature=0.3,
        api_key=settings.ZHIPU_API_KEY
    )
    
    paragraphs = text.split('\n\n')
    
    chunks = []
    current_chunk = ""
    
    for para in paragraphs:
        if len(current_chunk) + len(para) > chunk_size:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = para
        else:
            if current_chunk:
                current_chunk += "\n\n" + para
            else:
                current_chunk = para
    
    if current_chunk:
        chunks.append(current_chunk)
    
    print(f"   ğŸ“ å…± {len(chunks)} ä¸ªç¿»è¯‘å—")
    
    translated_chunks = []
    
    for i, chunk in enumerate(chunks, 1):
        print(f"   ğŸ”„ ç¿»è¯‘ä¸­... {i}/{len(chunks)}", end='\r')
        
        prompt = f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„ï¼Œç”¨ç©ºè¡Œåˆ†éš”æ®µè½
2. ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡è¡¨è¾¾
3. ä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§
4. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šï¼Œåªè¿”å›ç¿»è¯‘ç»“æœ

è‹±æ–‡åŸæ–‡ï¼š
{chunk}

ä¸­æ–‡ç¿»è¯‘ï¼š"""

        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            translated_text = response.content.strip()
            translated_chunks.append(translated_text)
        except Exception as e:
            logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
            print(f"\n   âŒ ç¿»è¯‘å— {i} å¤±è´¥: {e}")
            translated_chunks.append(chunk)
    
    print(f"   âœ… ç¿»è¯‘å®Œæˆï¼        ")
    
    return "\n\n".join(translated_chunks)


def create_word_document(video_data: dict, output_path: str, language: str):
    """åˆ›å»ºWordæ–‡æ¡£"""
    
    doc = Document()
    
    title = video_data.get('title', 'Untitled')
    
    doc_title = doc.add_heading(f'ã€{language}ã€‘{title}', 0)
    doc_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    lang_para = doc.add_paragraph()
    lang_run = lang_para.add_run(f'å­—å¹•è¯­è¨€: {language}')
    lang_run.bold = True
    lang_run.font.color.rgb = RGBColor(0, 102, 204)
    lang_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph('')
    
    info_table = doc.add_table(rows=6, cols=2)
    info_table.style = 'Table Grid'
    
    info_data = [
        ('é¢‘é“', video_data.get('channel', 'N/A')),
        ('æ’­æ”¾é‡', f"{video_data.get('view_count', 0):,}" if isinstance(video_data.get('view_count'), int) else str(video_data.get('view_count', 'N/A'))),
        ('ç‚¹èµæ•°', f"{video_data.get('like_count', 0):,}" if video_data.get('like_count') else 'N/A'),
        ('æ—¶é•¿', f"{video_data.get('duration', 0)} ç§’" if isinstance(video_data.get('duration'), int) else str(video_data.get('duration', 'N/A'))),
        ('é“¾æ¥', video_data.get('url', 'N/A')),
        ('åˆ†ææ—¥æœŸ', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    ]
    
    for i, (label, value) in enumerate(info_data):
        row = info_table.rows[i]
        row.cells[0].text = label
        row.cells[1].text = str(value)
    
    doc.add_paragraph('')
    doc.add_heading('è§†é¢‘æè¿°', level=1)
    description = video_data.get('description', 'æ— æè¿°')
    doc.add_paragraph(description[:1000] if len(description) > 1000 else description)
    
    transcript = video_data.get('transcript', '')
    if transcript:
        doc.add_page_break()
        doc.add_heading(f'å®Œæ•´å­—å¹• ({language})', level=1)
        
        processed_text = auto_sentence_break(transcript)
        paragraphs = processed_text.split('\n\n')
        
        for para_text in paragraphs:
            if para_text.strip():
                p = doc.add_paragraph()
                run = p.add_run(para_text.strip())
                
                if language == "ä¸­æ–‡":
                    set_chinese_font(run, 'SimSun', 12)
                else:
                    run.font.name = 'Times New Roman'
                    run.font.size = Pt(12)
                
                p.paragraph_format.first_line_indent = Pt(20)
                p.paragraph_format.space_after = Pt(8)
        
        doc.add_paragraph('')
        p = doc.add_paragraph()
        run = p.add_run(f'å­—å¹•æ€»é•¿åº¦: {len(transcript):,} å­—ç¬¦ | è¯­è¨€: {language}')
        run.italic = True
        run.font.color.rgb = RGBColor(128, 128, 128)
    
    doc.save(output_path)
    return output_path


def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "="*70)
    print("ğŸŒ å¼ºåˆ¶é‡æ–°ç¿»è¯‘è‹±æ–‡å­—å¹•ä¸ºä¸­æ–‡")
    print("="*70 + "\n")
    
    files_to_translate = [
        'data/youtube/video_pqzcCfUglws.json',
        'data/youtube/video_rEZukMdkQiA.json',
        'data/youtube/video_T5-re2X-YSY.json',
        'data/youtube/video_S15XpqbUFFA.json',
        'data/youtube/video_Pqftm3o5RdQ.json'
    ]
    
    output_dir = Path("data/youtube/word_documents")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    translated_files = []
    
    for i, json_file in enumerate(files_to_translate, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“„ [{i}/{len(files_to_translate)}] å¤„ç†: {Path(json_file).name}")
        print(f"{'='*70}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                video_data = json.load(f)
        except FileNotFoundError:
            print(f"   âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        title = video_data.get('title', 'Untitled')
        transcript = video_data.get('transcript', '')
        
        if not transcript:
            print(f"   âš ï¸  æ— å­—å¹•å†…å®¹ï¼Œè·³è¿‡")
            continue
        
        print(f"   ğŸ“¹ æ ‡é¢˜: {title[:50]}...")
        print(f"   ğŸ“ å­—å¹•é•¿åº¦: {len(transcript):,} å­—ç¬¦")
        
        chinese_chars = sum(1 for c in transcript[:500] if '\u4e00' <= c <= '\u9fff')
        total = len(transcript[:500].replace(' ', ''))
        is_already_chinese = chinese_chars / total > 0.3 if total > 0 else False
        
        if is_already_chinese:
            print(f"   âœ… å·²æ˜¯ä¸­æ–‡å†…å®¹ï¼Œè·³è¿‡ç¿»è¯‘")
            continue
        
        print(f"\n   ğŸ”„ å¼€å§‹ç¿»è¯‘...")
        translated_transcript = translate_text(transcript)
        
        video_data['transcript'] = translated_transcript
        video_data['language'] = "ä¸­æ–‡"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(video_data, f, ensure_ascii=False, indent=2)
        print(f"   ğŸ’¾ JSONå·²æ›´æ–°: {Path(json_file).name}")
        
        safe_title = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in title)
        safe_title = safe_title[:50]
        
        output_file = output_dir / f"Munger_ã€ä¸­æ–‡ã€‘_{safe_title}.docx"
        
        try:
            create_word_document(video_data, str(output_file), "ä¸­æ–‡")
            size = output_file.stat().st_size / 1024
            print(f"   âœ… Wordæ–‡æ¡£å·²ä¿å­˜: {output_file.name} ({size:.1f} KB)")
            translated_files.append((output_file, title, len(translated_transcript)))
        except Exception as e:
            print(f"   âŒ Wordè½¬æ¢å¤±è´¥: {e}")
    
    print("\n" + "="*70)
    print("âœ… ç¿»è¯‘å®Œæˆï¼")
    print("="*70)
    
    print(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {output_dir.absolute()}")
    
    if translated_files:
        print("\nğŸ“„ ç”Ÿæˆçš„ä¸­æ–‡Wordæ–‡æ¡£:")
        for file, title, length in translated_files:
            size = file.stat().st_size / 1024
            print(f"\n   ğŸ“„ {file.name}")
            print(f"      æ ‡é¢˜: {title[:50]}...")
            print(f"      å­—å¹•é•¿åº¦: {length:,} å­—ç¬¦ | æ–‡ä»¶å¤§å°: {size:.1f} KB")
    
    return translated_files


if __name__ == "__main__":
    main()
