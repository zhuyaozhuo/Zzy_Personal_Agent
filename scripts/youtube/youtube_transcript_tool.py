#!/usr/bin/env python3
"""
äº¤äº’å¼YouTubeå­—å¹•æå–å’ŒWordè½¬æ¢å·¥å…·
æ”¯æŒè¯­è¨€é€‰æ‹©ã€è‡ªåŠ¨æ–­å¥ã€Wordè½¬æ¢
"""

import asyncio
import sys
import os
import re
import json
from pathlib import Path
from datetime import datetime
from docx import Document
from docx.shared import Inches, Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from docx.oxml import OxmlElement

sys.path.insert(0, str(Path(__file__).parent))

from agents.youtube_agent import YouTubeAgent
from utils.logger import logger


def set_chinese_font(run, font_name='SimSun', font_size=12):
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    run.font.name = font_name
    run.font.size = Pt(font_size)
    r = run._element
    rPr = r.get_or_add_rPr()
    rFonts = OxmlElement('w:rFonts')
    rFonts.set(qn('w:eastAsia'), font_name)
    rPr.insert(0, rFonts)


def auto_sentence_break(text: str) -> str:
    """è‡ªåŠ¨æ–­å¥å¤„ç†"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'([.!?])\s+', r'\1\n\n', text)
    text = re.sub(r'([ã€‚ï¼ï¼Ÿ])', r'\1\n\n', text)
    text = re.sub(r'([,ï¼Œ])\s*', r'\1 ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def create_word_document(video_data: dict, output_path: str, language: str):
    """åˆ›å»ºWordæ–‡æ¡£"""
    
    doc = Document()
    
    title = video_data.get('title', 'Untitled')
    
    doc_title = doc.add_heading(f'ã€{language}ã€‘{title}', 0)
    doc_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    lang_para = doc.add_paragraph()
    lang_run = lang_para.add_run(f'å­—å¹•è¯­è¨€: {language}')
    lang_run.bold = True
    lang_run.font.color.rgb = RGBColor(0, 102, 204)
    lang_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph('')
    
    info_table = doc.add_table(rows=6, cols=2)
    info_table.style = 'Table Grid'
    
    info_data = [
        ('é¢‘é“', video_data.get('channel', 'N/A')),
        ('æ’­æ”¾é‡', f"{video_data.get('view_count', 0):,}" if isinstance(video_data.get('view_count'), int) else str(video_data.get('view_count', 'N/A'))),
        ('ç‚¹èµæ•°', f"{video_data.get('like_count', 0):,}" if video_data.get('like_count') else 'N/A'),
        ('æ—¶é•¿', f"{video_data.get('duration', 0)} ç§’" if isinstance(video_data.get('duration'), int) else str(video_data.get('duration', 'N/A'))),
        ('é“¾æ¥', video_data.get('url', 'N/A')),
        ('åˆ†ææ—¥æœŸ', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    ]
    
    for i, (label, value) in enumerate(info_data):
        row = info_table.rows[i]
        row.cells[0].text = label
        row.cells[1].text = str(value)
    
    doc.add_paragraph('')
    doc.add_heading('è§†é¢‘æè¿°', level=1)
    description = video_data.get('description', 'æ— æè¿°')
    doc.add_paragraph(description[:1000] if len(description) > 1000 else description)
    
    transcript = video_data.get('transcript', '')
    if transcript:
        doc.add_page_break()
        doc.add_heading(f'å®Œæ•´å­—å¹• ({language})', level=1)
        
        processed_text = auto_sentence_break(transcript)
        paragraphs = processed_text.split('\n\n')
        
        for para_text in paragraphs:
            if para_text.strip():
                p = doc.add_paragraph()
                run = p.add_run(para_text.strip())
                
                if language == "ä¸­æ–‡":
                    set_chinese_font(run, 'SimSun', 12)
                else:
                    run.font.name = 'Times New Roman'
                    run.font.size = Pt(12)
                
                p.paragraph_format.first_line_indent = Pt(20)
                p.paragraph_format.space_after = Pt(8)
        
        doc.add_paragraph('')
        p = doc.add_paragraph()
        run = p.add_run(f'å­—å¹•æ€»é•¿åº¦: {len(transcript):,} å­—ç¬¦ | è¯­è¨€: {language}')
        run.italic = True
        run.font.color.rgb = RGBColor(128, 128, 128)
    
    doc.save(output_path)
    return output_path


def ask_language_preference(available_languages: list) -> str:
    """è¯¢é—®ç”¨æˆ·é€‰æ‹©å­—å¹•è¯­è¨€"""
    
    print("\n" + "="*70)
    print("ğŸŒ è¯·é€‰æ‹©å­—å¹•è¯­è¨€")
    print("="*70)
    
    lang_options = {}
    option_num = 1
    
    preferred_langs = ['en', 'zh-CN', 'zh-TW', 'zh-Hans', 'zh-Hant']
    
    for lang in available_languages:
        code = lang.get('code', '')
        name = lang.get('name', code)
        is_generated = "è‡ªåŠ¨ç”Ÿæˆ" if lang.get('is_generated') else "äººå·¥"
        
        if code in ['en']:
            display_name = f"è‹±æ–‡ ({is_generated})"
        elif code in ['zh-CN', 'zh-Hans']:
            display_name = f"ä¸­æ–‡ç®€ä½“ ({is_generated})"
        elif code in ['zh-TW', 'zh-Hant']:
            display_name = f"ä¸­æ–‡ç¹ä½“ ({is_generated})"
        else:
            display_name = f"{name} ({is_generated})"
        
        lang_options[str(option_num)] = code
        print(f"   {option_num}. {display_name}")
        option_num += 1
    
    lang_options['en'] = 'en'
    lang_options['zh'] = 'zh-CN'
    lang_options['ä¸­æ–‡'] = 'zh-CN'
    lang_options['è‹±æ–‡'] = 'en'
    
    print("\n   ä¹Ÿå¯ä»¥ç›´æ¥è¾“å…¥: en, zh, ä¸­æ–‡, è‹±æ–‡")
    print("="*70)
    
    while True:
        choice = input("\nè¯·é€‰æ‹©è¯­è¨€ç¼–å·æˆ–è¾“å…¥è¯­è¨€ä»£ç : ").strip()
        
        if choice in lang_options:
            return lang_options[choice]
        elif choice in ['ä¸­æ–‡', 'è‹±æ–‡', 'en', 'zh', 'zh-CN', 'zh-TW']:
            return choice
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")


async def extract_transcripts_with_language(
    query: str = "Charlie Munger speech",
    max_results: int = 15,
    top_n: int = 3,
    language: str = None
):
    """
    æå–è§†é¢‘å­—å¹•å¹¶è½¬æ¢ä¸ºWordæ–‡æ¡£
    
    Args:
        query: æœç´¢å…³é”®è¯
        max_results: æœ€å¤§æœç´¢ç»“æœæ•°
        top_n: æå–å‰Nä¸ªè§†é¢‘
        language: å­—å¹•è¯­è¨€ (å¦‚æœä¸ºNoneåˆ™è¯¢é—®ç”¨æˆ·)
    """
    
    print("\n" + "="*70)
    print("ğŸ¬ YouTubeè§†é¢‘å­—å¹•æå–ä¸Wordè½¬æ¢")
    print("="*70 + "\n")
    
    agent = YouTubeAgent()
    
    print(f"ğŸ“¡ æ­£åœ¨æœç´¢: {query}...")
    search_result = agent._search_youtube(query, max_results=max_results)
    
    if not search_result.get("success"):
        print(f"âŒ æœç´¢å¤±è´¥: {search_result.get('error')}")
        return
    
    videos = search_result.get("videos", [])
    print(f"âœ… æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘\n")
    
    print("ğŸ“Š æ­£åœ¨è·å–è§†é¢‘è¯¦æƒ…...")
    videos_with_data = []
    for video in videos:
        details_result = agent._get_video_details(video["video_id"])
        if details_result.get("success"):
            video_data = {**video, **details_result.get("details", {})}
            views = video_data.get("view_count", video_data.get("views", 0))
            if isinstance(views, str):
                views = agent._parse_views(views)
            videos_with_data.append((views, video_data))
    
    videos_with_data.sort(key=lambda x: x[0], reverse=True)
    top_videos = videos_with_data[:top_n]
    
    print(f"\nâœ… æŒ‰æ’­æ”¾é‡æ’åºçš„å‰ {top_n} ä¸ªè§†é¢‘:\n")
    
    for i, (views, video) in enumerate(top_videos, 1):
        print(f"   {i}. {video['title'][:50]}... ({agent._format_number(views)} æ’­æ”¾)")
    
    if language is None:
        print("\nğŸ” æ­£åœ¨æ£€æµ‹ç¬¬ä¸€ä¸ªè§†é¢‘çš„å¯ç”¨å­—å¹•è¯­è¨€...")
        first_video_id = top_videos[0][1]["video_id"]
        lang_result = agent._list_available_transcripts(first_video_id)
        
        if lang_result.get("success"):
            available_languages = lang_result.get("languages", [])
            print(f"\nå¯ç”¨å­—å¹•è¯­è¨€: {len(available_languages)} ç§")
            language = ask_language_preference(available_languages)
        else:
            print("âš ï¸  æ— æ³•è·å–å¯ç”¨è¯­è¨€åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨è‹±æ–‡")
            language = "en"
    
    print(f"\nğŸ“ å°†ä½¿ç”¨è¯­è¨€: {language}")
    print("\n" + "="*70)
    
    output_dir = Path("data/youtube/word_documents")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    generated_files = []
    
    for i, (views, video) in enumerate(top_videos, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ“¹ è§†é¢‘ {i}: {video['title']}")
        print(f"{'='*70}")
        print(f"   ğŸ‘¤ é¢‘é“: {video.get('channel', 'N/A')}")
        print(f"   ğŸ‘ï¸  æ’­æ”¾é‡: {agent._format_number(views)}")
        print(f"   ğŸ”— é“¾æ¥: {video.get('url', 'N/A')}")
        
        print(f"\n   ğŸ“ æ­£åœ¨æå–å­—å¹• ({language})...")
        transcript_result = agent._get_video_transcript(
            video["video_id"], 
            language=language,
            auto_sentence_break=True
        )
        
        if transcript_result.get("success"):
            transcript = transcript_result.get("full_text", "")
            detected_lang = transcript_result.get("language", language)
            transcript_length = len(transcript)
            
            print(f"   âœ… å­—å¹•æå–æˆåŠŸï¼é•¿åº¦: {transcript_length:,} å­—ç¬¦")
            print(f"   ğŸŒ æ£€æµ‹è¯­è¨€: {detected_lang}")
            
            print(f"\n   ğŸ“„ å­—å¹•é¢„è§ˆ (å‰300å­—ç¬¦):")
            print(f"   {'-'*66}")
            preview = auto_sentence_break(transcript[:300])
            print(f"   {preview}...")
            print(f"   {'-'*66}")
            
            video["transcript"] = transcript
            video["transcript_length"] = transcript_length
            video["language"] = detected_lang
            video["view_count"] = views
            
            print(f"\n   ğŸ’¾ æ­£åœ¨è½¬æ¢ä¸ºWordæ–‡æ¡£...")
            
            safe_title = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in video.get('title', 'Untitled'))
            safe_title = safe_title[:50]
            
            output_file = output_dir / f"Video_{i}_ã€{detected_lang}ã€‘_{safe_title}.docx"
            
            try:
                created_file = create_word_document(video, str(output_file), detected_lang)
                size = Path(created_file).stat().st_size / 1024
                print(f"   âœ… Wordæ–‡æ¡£å·²ä¿å­˜: {output_file.name} ({size:.1f} KB)")
                generated_files.append((created_file, detected_lang, video['title']))
            except Exception as e:
                print(f"   âŒ Wordè½¬æ¢å¤±è´¥: {e}")
            
            json_file = output_dir.parent / f"video_{video['video_id']}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(video, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ“„ JSONæ•°æ®å·²ä¿å­˜: {json_file.name}")
            
        else:
            error = transcript_result.get("error", "æœªçŸ¥é”™è¯¯")
            print(f"   âš ï¸  å­—å¹•æå–å¤±è´¥: {error}")
            video["transcript"] = None
            video["view_count"] = views
    
    print("\n" + "="*70)
    print("âœ… ä»»åŠ¡å®Œæˆï¼")
    print("="*70)
    
    print(f"\nğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {output_dir.absolute()}")
    
    if generated_files:
        print("\nğŸ“„ ç”Ÿæˆçš„Wordæ–‡æ¡£:")
        for file, lang, title in generated_files:
            size = Path(file).stat().st_size / 1024
            print(f"   ğŸ“„ {Path(file).name}")
            print(f"      è¯­è¨€: {lang} | å¤§å°: {size:.1f} KB")
    
    return generated_files


async def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "="*70)
    print("ğŸ¬ YouTubeå­—å¹•æå–å·¥å…·")
    print("="*70)
    
    query = input("\nè¯·è¾“å…¥æœç´¢å…³é”®è¯ (é»˜è®¤: Charlie Munger speech): ").strip()
    if not query:
        query = "Charlie Munger speech"
    
    top_n_input = input("æå–å‰å‡ ä¸ªè§†é¢‘? (é»˜è®¤: 3): ").strip()
    top_n = int(top_n_input) if top_n_input.isdigit() else 3
    
    lang_input = input("å­—å¹•è¯­è¨€ (ä¸­æ–‡/è‹±æ–‡/en/zhï¼Œç•™ç©ºåˆ™è‡ªåŠ¨è¯¢é—®): ").strip()
    language = lang_input if lang_input else None
    
    await extract_transcripts_with_language(
        query=query,
        max_results=15,
        top_n=top_n,
        language=language
    )


if __name__ == "__main__":
    asyncio.run(main())
